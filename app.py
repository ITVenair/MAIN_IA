import streamlit as st
import google.generativeai as genai
import os
import PyPDF2 as pdf
import io # Para manejar los bytes del archivo subido

# --- ‚öôÔ∏è Configuraci√≥n de la P√°gina Streamlit ---
st.set_page_config(
    page_title="AsisBot Pro MAIN ‚ú®",
    page_icon="ü§ñ",
    layout="wide" # Usa el ancho completo de la p√°gina
)

# --- ‚ú® T√≠tulo y Encabezado ---
st.title("ü§ñ AsisBot Pro MAIN ‚ú®")
st.caption("Tu asistente inteligente potenciado por Gemini.")
st.divider() # L√≠nea divisoria para separar

# --- üîë Gesti√≥n de la API Key (igual que antes) ---
# M√©todo 1: Streamlit Secrets (Recomendado para despliegue)
google_api_key = st.secrets.get("GOOGLE_API_KEY")
# M√©todo 2: Variable de entorno (Pruebas locales)
if not google_api_key:
    google_api_key = os.getenv("GOOGLE_API_KEY")
# M√©todo 3: Input manual (Pruebas locales r√°pidas)
if not google_api_key:
    st.warning("üîë API Key no encontrada. Por favor, ingr√©sala en la barra lateral.")
    google_api_key = st.sidebar.text_input("Ingresa tu Google API Key:", type="password")

if not google_api_key:
    st.info("Ingresa tu Google API Key en la barra lateral para comenzar.")
    st.stop()

# --- üß† Configuraci√≥n del Modelo Gemini ---
try:
    genai.configure(api_key=google_api_key)
    model = genai.GenerativeModel('gemini-2.5-pro-exp-03-25')
    st.sidebar.success("‚úÖ Conectado a Gemini")
except Exception as e:
    st.error(f"‚ùå Error al configurar Gemini: {e}")
    st.stop()

# --- üìÑ Funci√≥n para Extraer Texto de Archivos ---
def extract_text_from_upload(uploaded_file):
    """Extrae texto de archivos TXT o PDF subidos."""
    text = ""
    try:
        if uploaded_file.type == "text/plain":
            # Leer archivo de texto
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            text = stringio.read()
        elif uploaded_file.type == "application/pdf":
            # Leer archivo PDF
            reader = pdf.PdfReader(uploaded_file)
            for page in reader.pages:
                text += page.extract_text() or "" # A√±adir texto de cada p√°gina
        else:
            st.sidebar.warning(f"Tipo de archivo no soportado: {uploaded_file.type}")
            return None # Retorna None si el tipo no es soportado
    except Exception as e:
        st.sidebar.error(f"Error al procesar el archivo {uploaded_file.name}: {e}")
        return None # Retorna None en caso de error
    return text

# --- üíæ Inicializaci√≥n del Historial y Estado en Session State ---
if "chat" not in st.session_state:
    # Instrucciones iniciales personalizadas
    initial_prompt = [
         {
            "role": "user",
            "parts": ["Eres 'AsisBot Pro', un asistente virtual avanzado para el departamento X. Ayudas con tareas, respondes preguntas sobre procesos y analizas documentos proporcionados por el usuario. Mant√©n un tono amigable, profesional y estructurado. Si te proporcionan contexto de un archivo, basa tu respuesta PRINCIPALMENTE en √©l. Si no sabes algo o no est√° en el contexto, ind√≠calo claramente."]
        },
        {
            "role": "model",
            "parts": ["¬°Hola! Soy AsisBot Pro ü§ñ. Estoy listo para ayudarte. Puedes chatear conmigo o subir un archivo TXT o PDF en la barra lateral y hacerme preguntas sobre √©l."]
        }
    ]
    st.session_state.chat = model.start_chat(history=initial_prompt)
    st.session_state.uploaded_file_text = None # Para guardar el texto del archivo
    st.session_state.uploaded_file_name = None # Para guardar el nombre del archivo
    print("Historial y estado inicializados.") # Debug

# --- üì§ Barra Lateral para Carga de Archivos y Opciones ---
st.sidebar.header("‚öôÔ∏è Opciones")

uploaded_file = st.sidebar.file_uploader(
    "üìÅ Carga un archivo (TXT o PDF)",
    type=["txt", "pdf"],
    help="Sube un documento para que pueda responder preguntas sobre su contenido."
)

if uploaded_file:
    # Procesa el archivo SOLO si es diferente al anterior o si no hay texto guardado
    if uploaded_file.name != st.session_state.get("uploaded_file_name") or not st.session_state.get("uploaded_file_text"):
        with st.sidebar:
            with st.spinner(f"‚è≥ Procesando '{uploaded_file.name}'..."):
                extracted_text = extract_text_from_upload(uploaded_file)
                if extracted_text is not None:
                    st.session_state.uploaded_file_text = extracted_text
                    st.session_state.uploaded_file_name = uploaded_file.name
                    st.success(f"‚úÖ Archivo '{uploaded_file.name}' procesado.")
                    st.caption("Ahora puedes hacer preguntas sobre este archivo en el chat.")
                else:
                    # Si la extracci√≥n falla, limpia el estado
                    st.session_state.uploaded_file_text = None
                    st.session_state.uploaded_file_name = None
                    st.error("No se pudo extraer texto del archivo.")

# Bot√≥n para limpiar el contexto del archivo
if st.session_state.get("uploaded_file_text"):
    st.sidebar.info(f"Archivo cargado: **{st.session_state.uploaded_file_name}**")
    if st.sidebar.button("üßπ Limpiar Contexto del Archivo"):
        st.session_state.uploaded_file_text = None
        st.session_state.uploaded_file_name = None
        st.sidebar.success("Contexto del archivo limpiado.")
        st.rerun() # Recarga para reflejar el cambio

# Bot√≥n para limpiar historial de chat
if st.sidebar.button("üóëÔ∏è Limpiar Historial de Chat"):
    initial_prompt = [ # Re-define por si acaso
         {
            "role": "user",
            "parts": ["Eres 'AsisBot Pro', un asistente virtual avanzado para MAIN. Ayudas con tareas, respondes preguntas sobre procesos y analizas documentos proporcionados por el usuario. Mant√©n un tono amigable, profesional y estructurado. Si te proporcionan contexto de un archivo, basa tu respuesta PRINCIPALMENTE en √©l. Si no sabes algo o no est√° en el contexto, ind√≠calo claramente."]
        },
        {
            "role": "model",
            "parts": ["¬°Hola! Soy AsisBot Pro ü§ñ. ¬øC√≥mo puedo ayudarte ahora?"]
        }
    ]
    st.session_state.chat = model.start_chat(history=initial_prompt)
    st.rerun()

st.sidebar.divider()
st.sidebar.markdown("Hecho con ‚ù§Ô∏è usando [Streamlit](https://streamlit.io) y [Google Gemini](https://ai.google.dev/)")

# --- üí¨ Interfaz Principal de Chat ---

# Mostrar mensajes anteriores
for message in st.session_state.chat.history:
    # Evita mostrar el prompt inicial del sistema si es muy largo o t√©cnico
    # O puedes personalizar qu√© mostrar aqu√≠
    is_system_prompt = "Eres 'AsisBot Pro'" in message.parts[0].text if message.parts else False
    if message.parts and message.parts[0].text and not is_system_prompt:
         with st.chat_message(message.role):
            st.markdown(message.parts[0].text)

# Input del usuario
user_prompt = st.chat_input("‚ùì Escribe tu pregunta o pide ayuda sobre el archivo cargado...")

if user_prompt:
    # Mostrar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # Preparar el prompt para Gemini
    context_to_send = ""
    if st.session_state.get("uploaded_file_text"):
        context_to_send = f"""
        --- INICIO DEL CONTEXTO DEL DOCUMENTO ({st.session_state.uploaded_file_name}) ---
        {st.session_state.uploaded_file_text[:8000]}
        --- FIN DEL CONTEXTO DEL DOCUMENTO ---

        Bas√°ndote **principalmente** en el contexto del documento proporcionado arriba, responde a la siguiente pregunta. Si la respuesta no est√° en el contexto, ind√≠calo.
        Pregunta: {user_prompt}
        """
        # Limitamos el contexto a 8000 caracteres para evitar exceder l√≠mites f√°cilmente
        # En una app real, se usar√≠an t√©cnicas m√°s avanzadas (chunking, embeddings)
        print(f"DEBUG: Enviando pregunta con contexto del archivo: {st.session_state.uploaded_file_name}") # Debug
    else:
        context_to_send = user_prompt # Sin contexto de archivo
        print("DEBUG: Enviando pregunta sin contexto de archivo.") # Debug

    # Enviar al modelo y mostrar respuesta
    try:
        response = st.session_state.chat.send_message(context_to_send, stream=True)
        with st.chat_message("model"): # Rol del asistente
            st.write_stream(response)
    except Exception as e:
        st.error(f"‚ùå Ocurri√≥ un error al contactar a Gemini: {e}")
